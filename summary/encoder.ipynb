{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'CudaTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-05773d1775cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#model = encoder.build_blank_model(opt, data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;31m# function encoder.build(opt, data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#    torch.setdefaulttensortype(\"torch.CudaTensor\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-05773d1775cd>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(opt, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCudaTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# dtype = torch.cuda.CudaTensor # Uncomment this to run on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderModel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'CudaTensor'"
     ]
    }
   ],
   "source": [
    "# --\n",
    "# --  Copyright (c) 2015, Facebook, Inc.\n",
    "# --  All rights reserved.\n",
    "# --\n",
    "# --  This source code is licensed under the BSD-style license found in the\n",
    "# --  LICENSE file in the root directory of this source tree. An additional grant\n",
    "# --  of patent rights can be found in the PATENTS file in the same directory.\n",
    "# --\n",
    "# --  Author: Alexander M Rush <srush@seas.harvard.edu>\n",
    "# --          Sumit Chopra <spchopra@fb.com>\n",
    "# --          Jason Weston <jase@fb.com>\n",
    "\n",
    "# local encoder = {}\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "def add_opts(cmd):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--encoderModel\", help=\"The encoder model to use.e.g. bow\")\n",
    "    parser.add_argument(\"--bowDim\", help=\"Article embedding size. e.g.50\")\n",
    "    parser.add_argument(\"--attenPool\", help=\"Attention model pooling size. e.g.5\")\n",
    "    parser.add_argument(\"--hiddenUnits\", help=\"Conv net encoder hidden units. e.g.1000\")\n",
    "    parser.add_argument(\"--kernelWidth\", help=\"Conv net encoder kernel width. e.g.5\")\n",
    "    args = parser.parse_args()\n",
    "    if args.encoderModel:\n",
    "        print(\"inf turned on\")\n",
    "        print(args.encoderModel)\n",
    "\n",
    "\n",
    "# function encoder.add_opts(cmd)\n",
    "#    cmd:option('-encoderModel', 'bow', \"The encoder model to use.\")\n",
    "#    cmd:option('-bowDim',      50, \"Article embedding size.\")\n",
    "#    cmd:option('-attenPool',    5, \"Attention model pooling size.\")\n",
    "#    cmd:option('-hiddenUnits', 1000, \"Conv net encoder hidden units.\")\n",
    "#    cmd:option('-kernelWidth', 5,    \"Conv net encoder kernel width.\")\n",
    "# end\n",
    "\n",
    "def build(opt,data):\n",
    "    #TODO: what is CudaTensor?????\n",
    "    #https://github.com/torch/torch7/blob/master/doc/utility.md#torch.setdefaulttensortype\n",
    "    #according to torch documentation, CudaTensor doesn't exist\n",
    "    \n",
    "    #pytorch:\n",
    "    #http://pytorch.org/docs/master/tensors.html\n",
    "    #http://pytorch.org/tutorials/beginner/pytorch_with_examples.html?highlight=tensor%20type#pytorch-tensors\n",
    "    \n",
    "    #dtype = torch.CudaTensor\n",
    "    # dtype = torch.cuda.CudaTensor # Uncomment this to run on GPU\n",
    "    if opt.encoderModel == \"none\":\n",
    "        #TODO: will work on this tomorrow\n",
    "        print(\"blank\")\n",
    "        #model = encoder.build_blank_model(opt, data)\n",
    "\n",
    "# function encoder.build(opt, data)\n",
    "#    torch.setdefaulttensortype(\"torch.CudaTensor\")\n",
    "#    local model = nil\n",
    "#    if opt.encoderModel == \"none\" then\n",
    "#       model = encoder.build_blank_model(opt, data)\n",
    "#    elseif opt.encoderModel == \"bow\" then\n",
    "#       model =  encoder.build_bow_model(opt, data)\n",
    "#    elseif opt.encoderModel == \"attenbow\" then\n",
    "#       model = encoder.build_attnbow_model(opt, data)\n",
    "#    elseif opt.encoderModel == \"conv\" then\n",
    "#       model = encoder.build_conv_model(opt, data)\n",
    "#    end\n",
    "#    torch.setdefaulttensortype(\"torch.DoubleTensor\")\n",
    "#    return model\n",
    "# end\n",
    "\n",
    "def build_blank_model(opt,data):\n",
    "    \n",
    "# function encoder.build_blank_model(opt, data)\n",
    "#    -- Ignores the article layer entirely (acts like LM).\n",
    "#    local lookup = nn.Identity()()\n",
    "#    local ignore1 = nn.Identity()()\n",
    "#    local ignore2 = nn.Identity()()\n",
    "#    local start = nn.SelectTable(3)({lookup, ignore1, ignore2})\n",
    "\n",
    "#    local mout = nn.MulConstant(0)(start)\n",
    "#    local encoder_mlp = nn.gModule({lookup, ignore1, ignore2}, {mout})\n",
    "#    encoder_mlp:cuda()\n",
    "#    return encoder_mlp\n",
    "# end\n",
    "\n",
    "\n",
    "# function encoder.build_bow_model(opt, data)\n",
    "#    print(\"Encoder model: Bag-of-Words\")\n",
    "\n",
    "#    -- BOW with mean on article.\n",
    "#    local lookup = nn.LookupTable(\n",
    "#       #data.article_data.dict.index_to_symbol,\n",
    "#       opt.bowDim)()\n",
    "\n",
    "#    -- Ignore the context.\n",
    "#    local ignore1 = nn.Identity()()\n",
    "#    local ignore2 = nn.Identity()()\n",
    "\n",
    "#    -- Ignores the context and position input.\n",
    "#    local start = nn.SelectTable(1)({lookup, ignore1, ignore2})\n",
    "#    local mout = nn.Linear(opt.bowDim, opt.bowDim)(\n",
    "#       nn.Mean(3)(nn.Transpose({2, 3})(start)))\n",
    "\n",
    "#    local encoder_mlp = nn.gModule({lookup, ignore1, ignore2}, {mout})\n",
    "#    encoder_mlp:cuda()\n",
    "\n",
    "#    return encoder_mlp\n",
    "# end\n",
    "\n",
    "\n",
    "# function encoder.build_conv_model(opt, data)\n",
    "#    -- Three layer thin convolutional architecture.\n",
    "#    print(\"Encoder model: Conv\")\n",
    "#    local V2 = #data.article_data.dict.index_to_symbol\n",
    "#    local nhid = opt.hiddenUnits\n",
    "\n",
    "#    -- Article embedding.\n",
    "#    local article_lookup = nn.LookupTable(V2, nhid)()\n",
    "\n",
    "#    -- Ignore the context.\n",
    "#    local ignore1 = nn.Identity()()\n",
    "#    local ignore2 = nn.Identity()()\n",
    "#    local start = nn.SelectTable(1)({article_lookup, ignore1, ignore2})\n",
    "#    local kwidth = opt.kernelWidth\n",
    "#    local model = nn.Sequential()\n",
    "#    model:add(nn.View(1, -1, nhid):setNumInputDims(2))\n",
    "#    model:add(cudnn.SpatialConvolution(1, nhid, nhid, kwidth, 1, 1, 0))\n",
    "#    model:add(cudnn.SpatialMaxPooling(1, 2, 1, 2))\n",
    "#    model:add(nn.Threshold())\n",
    "#    model:add(nn.Transpose({2,4}))\n",
    "\n",
    "#    -- layer 2\n",
    "#    model:add(cudnn.SpatialConvolution(1, nhid, nhid, kwidth, 1, 1, 0))\n",
    "#    model:add(nn.Threshold())\n",
    "#    model:add(nn.Transpose({2,4}))\n",
    "\n",
    "#    -- layer 3\n",
    "#    model:add(cudnn.SpatialConvolution(1, nhid, nhid, kwidth, 1, 1, 0))\n",
    "#    model:add(nn.View(nhid, -1):setNumInputDims(3))\n",
    "#    model:add(nn.Max(3))\n",
    "#    local done = nn.View(opt.hiddenUnits)(model(start))\n",
    "\n",
    "#    local mout = nn.Linear(opt.hiddenUnits, opt.embeddingDim)(done)\n",
    "\n",
    "#    local encoder_mlp = nn.gModule({article_lookup, ignore1, ignore2}, {mout})\n",
    "#    encoder_mlp.lookup = article_lookup.data.module\n",
    "#    encoder_mlp:cuda()\n",
    "#    return encoder_mlp\n",
    "# end\n",
    "\n",
    "\n",
    "# function encoder.build_attnbow_model(opt, data)\n",
    "#    print(\"Encoder model: BoW + Attention\")\n",
    "\n",
    "#    local D2 = opt.bowDim\n",
    "#    local N = opt.window\n",
    "#    local V = #data.title_data.dict.index_to_symbol\n",
    "#    local V2 = #data.article_data.dict.index_to_symbol\n",
    "\n",
    "#    -- Article Embedding.\n",
    "#    local article_lookup = nn.LookupTable(V2, D2)()\n",
    "\n",
    "#    -- Title Embedding.\n",
    "#    local title_lookup = nn.LookupTable(V, D2)()\n",
    "\n",
    "#    -- Size Lookup\n",
    "#    local size_lookup = nn.Identity()()\n",
    "\n",
    "#    -- Ignore size lookup to make NNGraph happy.\n",
    "#    local article_context = nn.SelectTable(1)({article_lookup, size_lookup})\n",
    "\n",
    "#    -- Pool article\n",
    "#    local pad = (opt.attenPool - 1) / 2\n",
    "#    local article_match = article_context\n",
    "\n",
    "#    -- Title context embedding.\n",
    "#    local title_context = nn.View(D2, 1)(\n",
    "#       nn.Linear(N * D2, D2)(nn.View(N * D2)(title_lookup)))\n",
    "\n",
    "#    -- Attention layer. Distribution over article.\n",
    "#    local dot_article_context = nn.MM()({article_match,\n",
    "#                                         title_context})\n",
    "\n",
    "#    -- Compute the attention distribution.\n",
    "#    local non_linearity = nn.SoftMax()\n",
    "#    local attention = non_linearity(nn.Sum(3)(dot_article_context))\n",
    "\n",
    "#    local process_article =\n",
    "#       nn.Sum(2)(nn.SpatialSubSampling(1, 1, opt.attenPool)(\n",
    "#                    nn.SpatialZeroPadding(0, 0, pad, pad)(\n",
    "#                       nn.View(1, -1, D2):setNumInputDims(2)(article_context))))\n",
    "\n",
    "#    -- Apply attention to the subsampled article.\n",
    "#    local mout = nn.Linear(D2, D2)(\n",
    "#       nn.Sum(3)(nn.MM(true, false)(\n",
    "#                    {process_article,\n",
    "#                     nn.View(-1, 1):setNumInputDims(1)(attention)})))\n",
    "\n",
    "#    -- Apply attention\n",
    "#    local encoder_mlp = nn.gModule({article_lookup, size_lookup, title_lookup},\n",
    "#       {mout})\n",
    "\n",
    "#    encoder_mlp:cuda()\n",
    "#    encoder_mlp.lookup = article_lookup.data.module\n",
    "#    encoder_mlp.title_lookup = title_lookup.data.module\n",
    "#    return encoder_mlp\n",
    "# end\n",
    "\n",
    "# return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
